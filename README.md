# Projeto Pr√°tico - Redund√¢ncia de Arquivos com Azure Data Factory

## üîé Introdu√ß√£o

Neste projeto, vamos aprender como criar um processo completo de **redund√¢ncia de arquivos** usando os recursos do **Microsoft Azure**. O foco est√° em usar o **Azure Data Factory** para mover dados de um banco de dados SQL local (on-premises) para o **Azure Data Lake**, salvando os dados em arquivos `.TXT` organizados por camadas como `raw` ou `bronze`.

---

## üåê Objetivo

- Criar um fluxo seguro de transfer√™ncia de dados
- Integrar ambientes locais ao Azure
- Utilizar o Data Factory para transformar e organizar dados
- Gerar arquivos de backup autom√°ticos e estruturados

---

## üîπ Arquitetura do Projeto

```
[SQL Server Local] ‚Üí [Integration Runtime] ‚Üí [Azure Data Factory] ‚Üí [Azure Data Lake - Blob Storage]
```

*Imagem ilustrativa aqui*

---

## üîß Passo a Passo do Projeto

### 1. Criando o Integration Runtime
- No Data Factory, acesse "Infraestrutura de Integra√ß√£o"
- Crie um novo Integration Runtime do tipo "Self-hosted"
- Instale o agente no servidor local

*Imagem ilustrativa aqui*

---

### 2. Conectando ao Banco de Dados SQL Local
- Em "Linked Services", crie uma conex√£o para seu SQL Server on-premises
- Use as credenciais corretas e selecione o IR criado

*Imagem ilustrativa aqui*

---

### 3. Conectando ao Azure Data Lake (Blob Storage)
- Ainda em "Linked Services", crie uma conex√£o com sua conta de armazenamento
- Gere as chaves no portal do Azure e cole no Data Factory

*Imagem ilustrativa aqui*

---

### 4. Criando os Datasets
- Crie um dataset para a tabela SQL de origem
- Crie um dataset para a pasta de destino no Blob Storage (formato `.txt`)

*Imagem ilustrativa aqui*

---

### 5. Criando o Pipeline de C√≥pia
- Crie um novo pipeline
- Adicione a atividade "Copy Data"
- Configure a origem e o destino com os datasets criados

*Imagem ilustrativa aqui*

---

### 6. Configurando o Layout dos Arquivos
- No destino, escolha o formato `.TXT`
- Defina a organiza√ß√£o em pastas: `/raw/ano=2025/mes=04/dia=10/`

*Imagem ilustrativa aqui*

---

### 7. Validando e Executando o Pipeline
- Clique em "Validar Tudo"
- Clique em "Publicar"
- Execute o pipeline manualmente ou configure um trigger

*Imagem ilustrativa aqui*

---

### 8. Analisando a Execu√ß√£o
- Veja os detalhes da execu√ß√£o do pipeline
- Verifique os arquivos criados no Blob Storage

*Imagem ilustrativa aqui*

---

## üßÆ O que foi aprendido

- Conectar Azure com recursos on-premises
- Criar pipelines no Azure Data Factory
- Transformar dados relacionais em arquivos estruturados
- Utilizar Blob Storage para backup
- Organizar arquivos em camadas (raw, bronze, etc)

---

## üåü Poss√≠veis Evolu√ß√µes

- Automatizar com triggers agendados
- Adicionar camada silver com dados limpos
- Criar relat√≥rios com Power BI
- Integrar com DataBricks ou Synapse

---

